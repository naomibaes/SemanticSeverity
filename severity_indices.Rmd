---
title: 'Semantic severity method'
author: "Naomi Baes"
output:
  html_document:
    df_print: paged
---
```{r, message = FALSE, warning = FALSE}
rm(list = ls()) # remove everything in global environment
cat("\014") # clear console
knitr::opts_chunk$set(echo = TRUE, include = TRUE) # set all chunks to show code (echo) and output (include) when running script

library(here) # detect root directory 
library(dplyr) # data manipulation
```
#### Import data.
Load [1] collocates of trauma in the psychology corpus, [2] Warriner rating statistics, and [3] trauma frequencies in the psychology corpus.
```{r}
# specify paths for datasets
coll_path <- here("data", "trauma_year_counts.csv")
warr_path <- here("data", "warriner_emotion_rat.csv")

# load data
coll_count <- read.csv(coll_path) # load [1] trauma collocates in psychology abstracts per year (1970-2017)
coll_unique <- coll_count %>% distinct(lemma) # create tibble containing initial list of trauma collocates

# load and prepare Warriner data
warr_rat <- read.csv(warr_path) %>% # load [2] Warriner norm ratings (1-9)
  rename(id = "X", lemma = "word") %>% # rename columns
  filter(lemma != "FALSE", lemma != "TRUE") %>% # remove non-values (#) 
  mutate(V.Mean.Sum.R = 9 - V.Mean.Sum) %>% # reverse score `V.Mean.Sum` so scores range from happy(1)-unhappy(9)
  select(c("id", "lemma", "V.Mean.Sum.R", "A.Mean.Sum")) # select relevant columns
```
#### Compute the semantic severity measure.
##### **1)** Prepare thedata.
```{r}
# create matched df
matches <- inner_join(warr_rat, coll_count, by = "lemma") # join "Warriner lemmas "warr_rat" by `lemma` column in "collocations df "coll_count" to obtain unique word matches
matches <- matches[, c(1, 2, 5, 6, 3, 4)] # reorder columns & rename data frame for data manipulation
df_word <- matches %>% mutate(AV.Mean.Sum=(A.Mean.Sum + V.Mean.Sum.R)) # merge mean valence and arousal ratings

# filter matched df from when all counts reach double digits
word_count <- df_word %>% group_by(year) %>% summarise(count = n_distinct(lemma)) %>% ungroup() # aggregate repet of matched lemmas by year
year_cutoff <- max(word_count$year[word_count$count < 10]) # find most recent year where `count` is less than 10
df_word <- df_word %>% filter(year > year_cutoff) %>% arrange(year) # use "year_cutoff" as the filter criteria
``` 

##### **2)** Compute repetition-weighted severity indices.
> *Weighted average severity* = sum(ratings * repetitions by lemma) by year/ sum(repetitions) by year

Compute outcome variable `sev_word` and the weighted average for its arousal (`aro_word`) and valence (`val_word`) components.
```{r, message = FALSE}
# Step 1. Weighted sum = Sum(sum of mean severity ratings * the repetition of each word near trauma)

df_word <- df_word %>% # compute the product of mean sum ratings (AV,A,V) * lemma repetitions
  mutate(AV.prod = (AV.Mean.Sum*repet),
         A.prod = (A.Mean.Sum*repet),  
         V.prod = (V.Mean.Sum.R*repet))

df_word <- df_word %>% # group by year and sum for each group (AV,A,V) (numerator)
  group_by(year) %>%
  mutate(sumAVprod.word = sum(AV.prod),
         sumAprod.word = sum(A.prod),
         sumVprod.word = sum(V.prod)) %>% 
  ungroup()

# Step 2. Standardize: Weighted average = sum of(repetition-weighted severity) by lemma/ sum(repetitions by year)

df_word <- df_word %>% # sum repetitions by year (denominator)  
  group_by(year) %>% 
  mutate(sum_repet_word = sum(repet)) %>% 
  ungroup()

df <- df_word %>% # compute standardization (for AV,A,V)
  group_by(year) %>%
  summarize(sev_word = (sumAVprod.word/sum_repet_word),
         aro_word = (sumAprod.word/sum_repet_word),  
         val_word = (sumVprod.word/sum_repet_word)) %>%
  distinct() %>%
  ungroup()
```
##### **3) **Create and save the final data frame.
```{r}
write.csv(df, file='df.csv') # save final df in working directory (should contain variables for statistical analysis)
```
#### End of method setup.